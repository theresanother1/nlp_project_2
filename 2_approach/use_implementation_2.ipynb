{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f3859",
   "metadata": {},
   "source": [
    "# Data Wrangling Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5b7dd039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_text_transform(df, column, numeric_entity_pattern=r'#(\\d+);'):\n",
    "    \"\"\"\n",
    "    Cleans and transforms the specified column in the DataFrame:\n",
    "    - Removes HTML tags.\n",
    "    - Unescapes HTML entities.\n",
    "    - Decodes URL-encoded characters.\n",
    "    - Replaces numeric character references (e.g., #36; to $).\n",
    "    - Removes common web-related tokens like 'http', 'www', 'href'.\n",
    "    - Removes weekdays (full and abbreviated names).\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        column (str): The column name to transform.\n",
    "        numeric_entity_pattern (str): Regex pattern for numeric entities.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the transformed column.\n",
    "    \"\"\"\n",
    "    def clean_and_replace(text):\n",
    "        # Remove HTML tags\n",
    "        soup = BeautifulSoup(StringIO(str(text)), features='html.parser')\n",
    "        clean_text = soup.get_text()\n",
    "        # Unescape HTML entities\n",
    "        clean_text = html.unescape(clean_text)\n",
    "        # Decode URL-encoded characters\n",
    "        clean_text = urllib.parse.unquote(clean_text)\n",
    "        # Replace numeric entities\n",
    "        def replace_entity(match):\n",
    "            return chr(int(match.group(1)))\n",
    "        clean_text = re.sub(numeric_entity_pattern, replace_entity, clean_text)\n",
    "        # Remove common web-related tokens and first-level domains\n",
    "        clean_text = re.sub(r'\\b(http|www|href|aspx|com|org|net|edu|gov|info|biz)\\b', '', clean_text, flags=re.IGNORECASE)\n",
    "        # Remove weekdays (full and abbreviations)\n",
    "        clean_text = re.sub(\n",
    "            r'\\b(Monday|Mon|Tuesday|Tue|Tues|Wednesday|Wed|Thursday|Thu|Thurs|Friday|Fri|Saturday|Sat|Sunday|Sun)\\b',\n",
    "            '', clean_text, flags=re.IGNORECASE)\n",
    "        # Remove extra whitespace\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "        return clean_text\n",
    "\n",
    "    df[column] = df[column].apply(clean_and_replace)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f6245ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_given_text(text):\n",
    "    # Use BeautifulSoup to remove HTML tags, StringIO to avoid warning about URL format due to \\ in text\n",
    "    soup = BeautifulSoup(StringIO(str(text)), features='html.parser')\n",
    "    clean_text = soup.get_text()\n",
    "\n",
    "    #clean_text = replace_numeric_entities(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "# trying to remove html character references and such - relevant for simpler encodings\n",
    "def remove_character_references(df):\n",
    "    df['Description'] = df['Description'].apply(clean_given_text)\n",
    "    df['Description'] = df['Description'].apply(html.unescape)\n",
    "    df['Description'] = df['Description'].apply(urllib.parse.unquote)\n",
    "\n",
    "    df['Title'] = df['Title'].apply(clean_given_text)\n",
    "    df['Title'] = df['Title'].apply(html.unescape)\n",
    "    df['Title'] = df['Title'].apply(urllib.parse.unquote)\n",
    "\n",
    "\n",
    "def replace_numeric_entities(text, pattern):\n",
    "    # Replace all matches in the text with its char representation\n",
    "    def replace_entity(match):\n",
    "        # Convert the numeric part to an integer and then to the corresponding character\n",
    "        return chr(int(match.group(1)))\n",
    "    return re.sub(pattern, replace_entity, text)\n",
    "\n",
    "def apply_transformation(df, pattern): \n",
    "    df['Description'] = df['Description'].apply(replace_numeric_entities, pattern=pattern)\n",
    "    df['Title'] = df['Title'].apply(replace_numeric_entities, pattern=pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5012693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title_based_on_source(df):\n",
    "    \"\"\"\n",
    "    This function cleans the 'Title' column of a DataFrame by removing any occurrence of the source text,\n",
    "    if it is enclosed in parentheses within the title.\n",
    "\n",
    "    For each row:\n",
    "      - It retrieves the 'Title' and 'source' values.\n",
    "      - If 'source' is valid (not null and a string), it creates a regex pattern that escapes\n",
    "        the source text enclosed in parentheses.\n",
    "      - It then removes the matched pattern from the Title.\n",
    "      - Finally, it normalizes extra whitespace in the title and updates the row.\n",
    "    \n",
    "    The updated DataFrame is returned after applying this cleaning function to every row.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        title = row.get('Title', '')\n",
    "        src = row.get('source', '')\n",
    "        #print(title)\n",
    "        #print(src)\n",
    "        \n",
    "        # Check if the source is a valid string.\n",
    "        if pd.notnull(src) and src.count(\" \") == 0:\n",
    "    \n",
    "            # Build the regex pattern to match the source when it appears enclosed in parentheses.\n",
    "            pattern = re.escape(f\"({src})\").replace(\"\\\\\", \"\")\n",
    "            #print(\"PAttern\", pattern)\n",
    "            # Remove the pattern from the title.\n",
    "            new_title = title.replace(pattern, \"\")\n",
    "            # Replace multiple whitespace with a single space and remove leading/trailing spaces.\n",
    "            new_title = re.sub(r'\\s+', ' ', new_title).strip()\n",
    "            # Update the Title in the row.\n",
    "            row.loc['Title'] = new_title\n",
    "            #print(row['Title'])\n",
    "        \n",
    "        return row\n",
    "    # Apply the process_row function to every row in the DataFrame.\n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "# Applying the function to news_df_test and printing the updated DataFrame.\n",
    "#news_df_test = clean_title_based_on_source(news_df_test)\n",
    "#print(news_df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d85fdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom list of countries using a hardcoded list\n",
    "custom_countries = [\n",
    "    \"Afghanistan\", \"Albania\", \"Algeria\", \"Andorra\", \"Angola\", \"Antigua and Barbuda\",\n",
    "    \"Argentina\", \"Armenia\", \"Australia\", \"Austria\", \"Azerbaijan\", \"Bahamas\", \"Bahrain\",\n",
    "    \"Bangladesh\", \"Barbados\", \"Belarus\", \"Belgium\", \"Belize\", \"Benin\", \"Bhutan\",\n",
    "    \"Bolivia\", \"Bosnia and Herzegovina\", \"Botswana\", \"Brazil\", \"Brunei\", \"Bulgaria\",\n",
    "    \"Burkina Faso\", \"Burundi\", \"CÃ´te d'Ivoire\", \"Cabo Verde\", \"Cambodia\", \"Cameroon\",\n",
    "    \"Canada\", \"Central African Republic\", \"Chad\", \"Chile\", \"China\", \"Colombia\",\n",
    "    \"Comoros\", \"Congo (Congo-Brazzaville)\", \"Costa Rica\", \"Croatia\", \"Cuba\", \"Cyprus\",\n",
    "    \"Czechia (Czech Republic)\", \"Democratic Republic of the Congo\", \"Denmark\", \"Djibouti\",\n",
    "    \"Dominica\", \"Dominican Republic\", \"Ecuador\", \"Egypt\", \"El Salvador\",\n",
    "    \"Equatorial Guinea\", \"Eritrea\", \"Estonia\", \"Eswatini (fmr. 'Swaziland')\",\n",
    "    \"Ethiopia\", \"Fiji\", \"Finland\", \"France\", \"Gabon\", \"Gambia\", \"Georgia\", \"Germany\",\n",
    "    \"Ghana\", \"Greece\", \"Grenada\", \"Guatemala\", \"Guinea\", \"Guinea-Bissau\", \"Guyana\",\n",
    "    \"Haiti\", \"Holy See\", \"Honduras\", \"Hungary\", \"Iceland\", \"India\", \"Indonesia\",\n",
    "    \"Iran\", \"Iraq\", \"Ireland\", \"Israel\", \"Italy\", \"Jamaica\", \"Japan\", \"Jordan\",\n",
    "    \"Kazakhstan\", \"Kenya\", \"Kiribati\", \"Kuwait\", \"Kyrgyzstan\", \"Laos\", \"Latvia\",\n",
    "    \"Lebanon\", \"Lesotho\", \"Liberia\", \"Libya\", \"Liechtenstein\", \"Lithuania\", \"Luxembourg\",\n",
    "    \"Madagascar\", \"Malawi\", \"Malaysia\", \"Maldives\", \"Mali\", \"Malta\", \"Marshall Islands\",\n",
    "    \"Mauritania\", \"Mauritius\", \"Mexico\", \"Micronesia\", \"Moldova\", \"Monaco\", \"Mongolia\",\n",
    "    \"Montenegro\", \"Morocco\", \"Mozambique\", \"Myanmar (formerly Burma)\", \"Namibia\",\n",
    "    \"Nauru\", \"Nepal\", \"Netherlands\", \"New Zealand\", \"Nicaragua\", \"Niger\", \"Nigeria\",\n",
    "    \"North Korea\", \"North Macedonia\", \"Norway\", \"Oman\", \"Pakistan\", \"Palau\",\n",
    "    \"Palestine State\", \"Panama\", \"Papua New Guinea\", \"Paraguay\", \"Peru\", \"Philippines\",\n",
    "    \"Poland\", \"Portugal\", \"Qatar\", \"Romania\", \"Russia\", \"Rwanda\",\n",
    "    \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\", \"Samoa\",\n",
    "    \"San Marino\", \"Sao Tome and Principe\", \"Saudi Arabia\", \"Senegal\", \"Serbia\",\n",
    "    \"Seychelles\", \"Sierra Leone\", \"Singapore\", \"Slovakia\", \"Slovenia\", \"Solomon Islands\",\n",
    "    \"Somalia\", \"South Africa\", \"South Korea\", \"South Sudan\", \"Spain\", \"Sri Lanka\",\n",
    "    \"Sudan\", \"Suriname\", \"Sweden\", \"Switzerland\", \"Syria\", \"Tajikistan\", \"Tanzania\",\n",
    "    \"Thailand\", \"Timor-Leste\", \"Togo\", \"Tonga\", \"Trinidad and Tobago\", \"Tunisia\",\n",
    "    \"Turkey\", \"Turkmenistan\", \"Tuvalu\", \"Uganda\", \"Ukraine\",\n",
    "    \"United Arab Emirates\", \"United Kingdom\", \"United States of America\", \"Uruguay\",\n",
    "    \"Uzbekistan\", \"Vanuatu\", \"Venezuela\", \"Vietnam\", \"Yemen\", \"Zambia\", \"Zimbabwe\"\n",
    "]\n",
    "\n",
    "def move_country_from_source_to_location(df, countries=custom_countries):\n",
    "    \"\"\"\n",
    "    Searches for any country names within the 'source' column based on a given list\n",
    "    and moves it to the 'location' column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with at least a 'source' column and an optional 'location' column.\n",
    "        countries (list): List of country names to search for (default is custom_countries).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the country name moved from 'source' to 'location'.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        source_text = row.get('source')\n",
    "        location_text = row.get('location')\n",
    "        \n",
    "        # Skip processing if source_text is not valid.\n",
    "        if pd.isna(source_text) or not isinstance(source_text, str):\n",
    "            return row\n",
    "        \n",
    "        new_source = source_text\n",
    "        found_country = None\n",
    "        \n",
    "        # Check for any country present in the custom list.\n",
    "        for country in countries:\n",
    "            pattern = r'\\b' + re.escape(country) + r'\\b'\n",
    "            match = re.search(pattern, new_source, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                found_country = match.group(0)\n",
    "                new_source = re.sub(pattern, \"\", new_source, flags=re.IGNORECASE).strip()\n",
    "                break  # Stop after the first match\n",
    "        \n",
    "        # Update source with cleaned value (if empty, set to None)\n",
    "        row['source'] = new_source if new_source != \"\" else None\n",
    "        \n",
    "        # Update location with the found country if applicable.\n",
    "        if found_country:\n",
    "            if pd.isna(location_text) or location_text.strip() == \"\":\n",
    "                row['location'] = found_country\n",
    "            else:\n",
    "                row['location'] = f\"{location_text.strip()} {found_country}\"\n",
    "        return row\n",
    "\n",
    "    return df.apply(process_row, axis=1)\n",
    "\n",
    "# Apply the function to the test DataFrame and display updated head.\n",
    "#news_df_test = move_country_from_source_to_location(news_df_test)\n",
    "#print(news_df_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cbda9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_source_location(df):\n",
    "    \"\"\"\n",
    "    Process the 'source' column of a DataFrame to potentially split it into two parts:\n",
    "    the actual source and a location extracted from source text based on various patterns.\n",
    "    \n",
    "    Steps:\n",
    "      1. Checks if a 'source' column exists. If not, prints an error and returns the DataFrame unchanged.\n",
    "      2. Ensures a 'location' column exists by creating it with default NaN values if missing.\n",
    "      3. Defines several regex patterns to match common source formatting,\n",
    "         including adaptations:\n",
    "           - Abbreviation pattern: if the 'source' text starts with terms like \"Calif.\" or \"Flo.\",\n",
    "             then that word is moved to the 'location' column.\n",
    "           - Dot pattern: if the 'source' begins with a word ending in a dot (excluding first-level domain names)\n",
    "             then that word is moved to the 'location' column.\n",
    "      4. Applies these patterns to each row of the 'source' column using the process_text helper.\n",
    "      5. For entries where a location was extracted, the 'source' column is updated to the new source value,\n",
    "         and the new information is stored in the 'location' column.\n",
    "      6. Finally, the columns are reordered so that 'location' appears as the third column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame that must contain a 'source' column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with potentially restructured 'source' and extracted 'location'.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'source' not in df.columns:\n",
    "        print(\"Column 'source' does not exist in this DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    if 'location' not in df.columns:\n",
    "        df['location'] = np.nan\n",
    "\n",
    "    pattern_paren = r\"^\\s*([A-Z ]+)\\s*\\(([^)]+)\\)\"\n",
    "    pattern_comma = r\"^\\s*([A-Z ]+),\\s*(.+)$\"\n",
    "\n",
    "    def process_text(text):\n",
    "        \"\"\"\n",
    "        Applies regex checks to extract potential new 'source' and 'location' values.\n",
    "        \n",
    "        Matching order:\n",
    "          1. Website pattern: e.g., 'SPACE.com' splits domain from following text.\n",
    "          2. Abbreviation pattern: if the source starts with terms like \"Calif.\" or \"Flo.\"\n",
    "             then that term is moved to 'location'.\n",
    "          3. Adapted Dot pattern: if the source starts with a word ending with a '.'\n",
    "             that is not immediately followed by a common first-level-domain,\n",
    "             then the initial word is moved to 'location'.\n",
    "          4. Combo pattern: If text contains a comma before parentheses, uses the parenthesized text as source.\n",
    "          5. Parentheses pattern: Extracts text inside parentheses after uppercase letters.\n",
    "          6. Comma pattern: Alternate extraction with a comma.\n",
    "          7. Two capitalized words: Uses them as the location.\n",
    "          8. Fallback: Extracts a single all-caps word (more than three characters) as location.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            # Website pattern\n",
    "            pattern_website = r\"^\\s*([A-Za-z]+\\.(?:com|org|net|edu|gov|info|biz))\\s*(.*)$\"\n",
    "            match_website = re.match(pattern_website, text)\n",
    "            if match_website:\n",
    "                source_val = match_website.group(1).strip()\n",
    "                location_val = match_website.group(2).strip() if match_website.group(2) else None\n",
    "                return source_val, location_val\n",
    "\n",
    "            # Abbreviation pattern\n",
    "            pattern_abbrev = r'^\\s*((?:Calif\\.|Flo\\.))\\s+(.*)$'\n",
    "            match_abbrev = re.match(pattern_abbrev, text)\n",
    "            if match_abbrev:\n",
    "                return match_abbrev.group(2).strip(), match_abbrev.group(1).strip()\n",
    "\n",
    "            # Adapted Dot pattern:\n",
    "            # Matches a leading word ending with a period that is NOT followed by a first-level-domain.\n",
    "            match_dot = re.match(r'^\\s*([\\w\\-]+\\.)(?!\\s*(?:com|org|net|edu|gov|info|biz)\\b)(.*)$', text)\n",
    "            if match_dot:\n",
    "                # Here, the part ending with the dot is moved to 'location' \n",
    "                # while the rest of the text becomes the new source.\n",
    "                return match_dot.group(2).strip(), match_dot.group(1).strip()\n",
    "\n",
    "            # Combo pattern: text with parentheses at the end and a comma before.\n",
    "            pattern_combo = r\"^\\s*(.+?)\\s*\\(([^()]+)\\)\\s*$\"\n",
    "            match_combo = re.match(pattern_combo, text)\n",
    "            if match_combo:\n",
    "                if \",\" in match_combo.group(1):\n",
    "                    return match_combo.group(2).strip(), match_combo.group(1).strip()\n",
    "\n",
    "            # Parentheses pattern.\n",
    "            match = re.match(pattern_paren, text)\n",
    "            if match:\n",
    "                return match.group(2).strip(), match.group(1).strip()\n",
    "\n",
    "            # Comma pattern.\n",
    "            match2 = re.match(pattern_comma, text)\n",
    "            if match2:\n",
    "                return match2.group(2).strip(), match2.group(1).strip()\n",
    "\n",
    "            # Look for two consecutive capitalized words.\n",
    "            match_two = re.search(r'\\b([A-Z]{2,})\\s+([A-Z]{2,})\\b', text)\n",
    "            if match_two:\n",
    "                location = f\"{match_two.group(1)} {match_two.group(2)}\"\n",
    "                new_text = re.sub(re.escape(match_two.group(0)), '', text).strip()\n",
    "                return new_text, location\n",
    "\n",
    "            # Fallback: single all-caps word longer than three characters.\n",
    "            candidates = re.findall(r'\\b[A-Z]{4,}\\b', text)\n",
    "            if candidates:\n",
    "                location = candidates[0]\n",
    "                new_text = re.sub(r'\\b' + re.escape(location) + r'\\b', '', text).strip()\n",
    "                return new_text, location\n",
    "        return text, None\n",
    "\n",
    "    processed = df[\"source\"].apply(lambda x: process_text(x))\n",
    "    new_sources = processed.apply(lambda t: t[0])\n",
    "    locations = processed.apply(lambda t: t[1])\n",
    "\n",
    "    mask = locations.notna()\n",
    "    df.loc[mask, \"source\"] = new_sources[mask]\n",
    "    df.loc[mask, \"location\"] = locations[mask]\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    if \"location\" in cols:\n",
    "        cols.remove(\"location\")\n",
    "        cols.insert(2, \"location\")\n",
    "        df = df[cols]\n",
    "        \n",
    "    df[\"location\"] = df[\"location\"].astype('object')\n",
    "    \n",
    "    return df\n",
    "\n",
    "#news_df_test = restructure_source_location(news_df_test)\n",
    "#print(news_df_test.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a1edc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_source_from_description(df):\n",
    "    \"\"\"\n",
    "    Extracts a source string from the 'Description' column of the DataFrame and updates the DataFrame accordingly.\n",
    "\n",
    "    The function searches for separators in the beginning of the description as follows:\n",
    "    - If a double hyphen (\" -- \") is found within the first 35 characters, the part before it is\n",
    "      considered the source and the rest is kept as the new description.\n",
    "    - Otherwise, if a single hyphen (\" - \") is found within the first 30 characters, the extraction\n",
    "      is performed similarly.\n",
    "      \n",
    "    After processing, a new column 'source' is created (or updated) and the DataFrame columns are reordered\n",
    "    so that 'source' appears as the second column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing a 'Description' column to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with the extracted 'source' and modified 'Description'.\n",
    "    \"\"\"\n",
    "    def process_row(row):\n",
    "        desc = row['Description']\n",
    "        source_str = None  # Default value if no source is found.\n",
    "        new_desc = desc    # By default, keep the original description\n",
    "\n",
    "        # Check if a double hyphen (\" -- \") is found in the first 35 characters.\n",
    "        if ' -- ' in desc[:35]:\n",
    "            parts = desc.split('--', 1)  # Split only at the first occurrence.\n",
    "            source_str = parts[0].strip()  # Extract and remove extra whitespace from source.\n",
    "            new_desc = parts[1].strip()    # The remainder becomes the new description.\n",
    "        # If the double hyphen pattern wasn't found, check for a single hyphen (\" - \")\n",
    "        # in the first 30 characters and do a similar extraction.\n",
    "        elif ' - ' in desc[:30]:\n",
    "            parts = desc.split('-', 1)\n",
    "            source_str = parts[0].strip()\n",
    "            new_desc = parts[1].strip()\n",
    "\n",
    "        # Update the row with the extracted source and the cleaned description.\n",
    "        row['source'] = source_str\n",
    "        row['Description'] = new_desc\n",
    "        return row\n",
    "\n",
    "    # Apply the processing function to every row in the DataFrame.\n",
    "    df = df.apply(process_row, axis=1)\n",
    "\n",
    "    # Reorder columns so that 'source' is the second column.\n",
    "    cols = df.columns.tolist()\n",
    "    if 'source' in cols:\n",
    "        cols.remove('source')\n",
    "        cols.insert(1, 'source')\n",
    "        df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to the DataFrame and display the first 100 rows.\n",
    "#news_df_test = extract_source_from_description(news_df_test)\n",
    "#print(news_df_test.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fd269",
   "metadata": {},
   "source": [
    "# Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e260e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    1: \"World\",\n",
    "    2: \"Sports\", \n",
    "    3: \"Business\",\n",
    "    4: \"Science/Tech\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1e1af6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train DataFrame Shape: (120000, 3)\n",
      "Train DataFrame:\n",
      "|    |   Class Index | Title                                                                     | Description                                                                                                                                                                                                            |\n",
      "|---:|--------------:|:--------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 |             3 | Wall St. Bears Claw Back Into the Black (Reuters)                         | Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.                                                                                                                         |\n",
      "|  1 |             3 | Carlyle Looks Toward Commercial Aerospace (Reuters)                       | Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market. |\n",
      "|  2 |             3 | Oil and Economy Cloud Stocks' Outlook (Reuters)                           | Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.                               |\n",
      "|  3 |             3 | Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)              | Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.                    |\n",
      "|  4 |             3 | Oil prices soar to all-time record, posing new menace to US economy (AFP) | AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.                                                       |\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test DataFrame Shape: (7600, 3)\n",
      "\n",
      "Test DataFrame:\n",
      "|    |   Class Index | Title                                                                                  | Description                                                                                                                                                                                                                                                                      |\n",
      "|---:|--------------:|:---------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 |             3 | Fears for T N pension after talks                                                      | Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.                                                                                                                                                  |\n",
      "|  1 |             4 | The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) | SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.                                       |\n",
      "|  2 |             4 | Ky. Company Wins Grant to Study Peptides (AP)                                          | AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.                                                           |\n",
      "|  3 |             4 | Prediction Unit Helps Forecast Wildfires (AP)                                          | AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar. |\n",
      "|  4 |             4 | Calif. Aims to Limit Farm-Related Smog (AP)                                            | AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.                                                                                              |\n"
     ]
    }
   ],
   "source": [
    "test_data_fp = '/home/manndo/Projects/NLP/Project2/nlp_project_2/data/test.csv'\n",
    "train_data_pf = '/home/manndo/Projects/NLP/Project2/nlp_project_2/data/train.csv'\n",
    "\n",
    "test_df = pd.read_csv(test_data_fp)\n",
    "train_df = pd.read_csv(train_data_pf)\n",
    "\n",
    "\n",
    "print(\"\\nTrain DataFrame Shape:\", train_df.shape)\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df.head().to_markdown())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Test DataFrame Shape:\", test_df.shape)\n",
    "print(\"\\nTest DataFrame:\")\n",
    "print(test_df.head().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "65e3a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned data files...\n"
     ]
    }
   ],
   "source": [
    "def get_cleaned_fp(fp):\n",
    "    base, ext = os.path.splitext(fp)\n",
    "    return f\"{base}_cleaned.csv\"\n",
    "\n",
    "# Prepare file paths for cleaned data\n",
    "train_cleaned_fp = get_cleaned_fp(train_data_pf)\n",
    "test_cleaned_fp = get_cleaned_fp(test_data_fp)\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "# Check if cleaned files exist\n",
    "if os.path.exists(train_cleaned_fp) and os.path.exists(test_cleaned_fp):\n",
    "    print(\"Loading cleaned data files...\")\n",
    "    df_dict['train'] = pd.read_csv(train_cleaned_fp)\n",
    "    df_dict['test'] = pd.read_csv(test_cleaned_fp)\n",
    "else:\n",
    "    print(\"Processing and saving cleaned data files...\")\n",
    "    df_dict['train'] = pd.read_csv(train_data_pf)\n",
    "    df_dict['test'] = pd.read_csv(test_data_fp)\n",
    "    for key in df_dict:\n",
    "        df_dict[key] = full_text_transform(df_dict[key], \"Description\")\n",
    "        df_dict[key] = extract_source_from_description(df_dict[key])\n",
    "        df_dict[key] = clean_title_based_on_source(df_dict[key])\n",
    "        df_dict[key] = restructure_source_location(df_dict[key])\n",
    "        df_dict[key] = move_country_from_source_to_location(df_dict[key])\n",
    "    df_dict['train'].to_csv(train_cleaned_fp, index=False)\n",
    "    df_dict['test'].to_csv(test_cleaned_fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "08af03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Train DataFrame Shape: (120000, 5)\n",
      "Cleaned Train DataFrame:\n",
      "|    |   Class Index | source   |   location | Title                                                               | Description                                                                                                                                                                                                  |\n",
      "|---:|--------------:|:---------|-----------:|:--------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 |             3 | Reuters  |        nan | Wall St. Bears Claw Back Into the Black                             | Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.                                                                                                                         |\n",
      "|  1 |             3 | Reuters  |        nan | Carlyle Looks Toward Commercial Aerospace                           | Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market. |\n",
      "|  2 |             3 | Reuters  |        nan | Oil and Economy Cloud Stocks' Outlook                               | Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.                               |\n",
      "|  3 |             3 | Reuters  |        nan | Iraq Halts Oil Exports from Main Southern Pipeline                  | Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on .                            |\n",
      "|  4 |             3 | AFP      |        nan | Oil prices soar to all-time record, posing new menace to US economy | Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.                                                   |\n",
      "\n",
      "==================================================\n",
      "\n",
      "Cleaned Test DataFrame Shape: (7600, 5)\n",
      "\n",
      "Cleaned Test DataFrame:\n",
      "|    |   Class Index | source     | location      | Title                                                                                  | Description                                                                                                                                                                                                                                                                 |\n",
      "|---:|--------------:|:-----------|:--------------|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "|  0 |             3 | nan        | nan           | Fears for T N pension after talks                                                      | Unions representing workers at Turner Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.                                                                                                                                               |\n",
      "|  1 |             4 | - TORONTO, | SPACE. Canada | The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com) | A second\\team of rocketeers competing for the $10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.                                                                     |\n",
      "|  2 |             4 | AP         | nan           | Ky. Company Wins Grant to Study Peptides                                               | A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.                                                           |\n",
      "|  3 |             4 | AP         | nan           | Prediction Unit Helps Forecast Wildfires                                               | It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar. |\n",
      "|  4 |             4 | AP         | nan           | Calif. Aims to Limit Farm-Related Smog                                                 | Southern California's smog-fighting agency went after emissions of the bovine variety , adopting the nation's first rules to reduce air pollution from dairy cow manure.                                                                                                    |\n"
     ]
    }
   ],
   "source": [
    "train_df = df_dict['train']\n",
    "test_df = df_dict['test']\n",
    "\n",
    "print(\"\\nCleaned Train DataFrame Shape:\", train_df.shape)\n",
    "print(\"Cleaned Train DataFrame:\")\n",
    "print(train_df.head().to_markdown())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Cleaned Test DataFrame Shape:\", test_df.shape)\n",
    "print(\"\\nCleaned Test DataFrame:\")\n",
    "print(test_df.head().to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d4a9e245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "new year world game york season company time night software\n",
      "Topic #1:\n",
      "quot said says called yesterday told like saying coach just\n",
      "Topic #2:\n",
      "fullquote stocks reuters target investor ticker quickinfo oil percent prices\n",
      "Topic #3:\n",
      "said president minister iraq government people officials killed prime united\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(train_df['Description'])\n",
    "\n",
    "nmf = NMF(n_components=4, random_state=42) \n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[-10:][::-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "712c35ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA shape: (120000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Transform the descriptions using the existing vectorizer\n",
    "X_tfidf = vectorizer.transform(train_df['Description'])\n",
    "\n",
    "# Fit TruncatedSVD for LSA (choose n_components as needed, e.g., 100)\n",
    "lsa = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "# X_lsa now contains the LSA-reduced features for each document\n",
    "print(\"LSA shape:\", X_lsa.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
